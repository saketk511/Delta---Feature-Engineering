{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4126a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://anu9rng:****@rb-artifactory.bosch.com/artifactory/api/pypi/python-virtual/simple\n",
      "Requirement already satisfied: gensim in c:\\users\\usm8kor\\appdata\\roaming\\python\\python39\\site-packages (4.3.3)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18.5 in c:\\users\\usm8kor\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.23.5)\n",
      "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in c:\\users\\usm8kor\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.13.1)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\usm8kor\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (7.0.4)\n",
      "Requirement already satisfied: wrapt in c:\\program files\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.12.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec685991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Date_of_Journey</th>\n",
       "      <th>Source</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Route</th>\n",
       "      <th>Dep_Time</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Total_Stops</th>\n",
       "      <th>Additional_Info</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>24/03/2019</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>BLR → DEL</td>\n",
       "      <td>22:20</td>\n",
       "      <td>01:10 22 Mar</td>\n",
       "      <td>2h 50m</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>3897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air India</td>\n",
       "      <td>1/05/2019</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → IXR → BBI → BLR</td>\n",
       "      <td>05:50</td>\n",
       "      <td>13:15</td>\n",
       "      <td>7h 25m</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>No info</td>\n",
       "      <td>7662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jet Airways</td>\n",
       "      <td>9/06/2019</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Cochin</td>\n",
       "      <td>DEL → LKO → BOM → COK</td>\n",
       "      <td>09:25</td>\n",
       "      <td>04:25 10 Jun</td>\n",
       "      <td>19h</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>No info</td>\n",
       "      <td>13882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>12/05/2019</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → NAG → BLR</td>\n",
       "      <td>18:05</td>\n",
       "      <td>23:30</td>\n",
       "      <td>5h 25m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>6218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>01/03/2019</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>BLR → NAG → DEL</td>\n",
       "      <td>16:50</td>\n",
       "      <td>21:35</td>\n",
       "      <td>4h 45m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>13302</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Airline Date_of_Journey    Source Destination                  Route  \\\n",
       "0       IndiGo      24/03/2019  Banglore   New Delhi              BLR → DEL   \n",
       "1    Air India       1/05/2019   Kolkata    Banglore  CCU → IXR → BBI → BLR   \n",
       "2  Jet Airways       9/06/2019     Delhi      Cochin  DEL → LKO → BOM → COK   \n",
       "3       IndiGo      12/05/2019   Kolkata    Banglore        CCU → NAG → BLR   \n",
       "4       IndiGo      01/03/2019  Banglore   New Delhi        BLR → NAG → DEL   \n",
       "\n",
       "  Dep_Time  Arrival_Time Duration Total_Stops Additional_Info  Price  \n",
       "0    22:20  01:10 22 Mar   2h 50m    non-stop         No info   3897  \n",
       "1    05:50         13:15   7h 25m     2 stops         No info   7662  \n",
       "2    09:25  04:25 10 Jun      19h     2 stops         No info  13882  \n",
       "3    18:05         23:30   5h 25m      1 stop         No info   6218  \n",
       "4    16:50         21:35   4h 45m      1 stop         No info  13302  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'Data_Train.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f4a70d",
   "metadata": {},
   "source": [
    "Column-wise Transformations:\n",
    "\n",
    "    Airline:\n",
    "        Transformation: One-Hot Encoding (OHE) within categorical_pipeline\n",
    "    Date_of_Journey:\n",
    "        Transformation: Split into Journey_Day and Journey_Month (then dropped)\n",
    "    Source:\n",
    "        Transformation: TF-IDF Vectorization within tfidf_pipeline\n",
    "    Destination:\n",
    "        Transformation: TF-IDF Vectorization within tfidf_pipeline\n",
    "    Route:\n",
    "        Transformation: One-Hot Encoding (OHE) within categorical_pipeline\n",
    "    Dep_Time:\n",
    "        Transformation: Split into Dep_Hour and Dep_Minute (then dropped)\n",
    "    Arrival_Time:\n",
    "        Transformation: Split into Arrival_Hour and Arrival_Minute (then dropped)\n",
    "    Duration:\n",
    "        Transformation: Converted to Duration_Minutes (then dropped)\n",
    "    Total_Stops:\n",
    "        Transformation: Label Encoding\n",
    "    Additional_Info:\n",
    "        Transformation: Word2Vec Embedding into Additional_Info_Word2Vec (then dropped)\n",
    "    Price:\n",
    "        Transformation: Target Column (No Transformation)\n",
    "\n",
    "Additional Information:\n",
    "\n",
    "    Numerical Columns (Journey_Day, Journey_Month, Dep_Hour, Dep_Minute, Arrival_Hour, Arrival_Minute, Duration_Minutes, Total_Stops):\n",
    "        Transformation: Imputation with SimpleImputer, followed by scaling with StandardScaler within numerical_pipeline.\n",
    "    Categorical Columns (Airline, Route):\n",
    "        Transformation: Imputation with SimpleImputer, followed by One-Hot Encoding within categorical_pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6ff7175",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model R^2 score: -4.689424679936029e+16\n",
      "Transformed output:\n",
      "[[ 1.22840525 -1.46907017  1.65637489 -0.23582949 -1.79939492 -0.89024038\n",
      "  -0.92856352  1.40623786  0.          0.          0.          1.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          1.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          1.          0.          0.          0.          0.\n",
      "   0.          0.          0.60090709  0.          0.          0.79931888]]\n",
      "Shape of transformed data: (1, 156)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Feature Engineering: Splitting Date_of_Journey into day, month, year\n",
    "df['Journey_Day'] = pd.to_datetime(df['Date_of_Journey'], format='%d/%m/%Y').dt.day\n",
    "df['Journey_Month'] = pd.to_datetime(df['Date_of_Journey'], format='%d/%m/%Y').dt.month\n",
    "\n",
    "# Feature Engineering: Extracting hours and minutes from Dep_Time and Arrival_Time\n",
    "df['Dep_Hour'] = pd.to_datetime(df['Dep_Time']).dt.hour\n",
    "df['Dep_Minute'] = pd.to_datetime(df['Dep_Time']).dt.minute\n",
    "df['Arrival_Hour'] = pd.to_datetime(df['Arrival_Time']).dt.hour\n",
    "df['Arrival_Minute'] = pd.to_datetime(df['Arrival_Time']).dt.minute\n",
    "\n",
    "# Feature Engineering: Splitting Duration into hours and minutes\n",
    "duration_split = df['Duration'].str.extract(r'(?:(\\d+)h)?\\s*(?:(\\d+)m)?')\n",
    "df['Duration_Minutes'] = duration_split[0].fillna(0).astype(int) * 60 + duration_split[1].fillna(0).astype(int)\n",
    "\n",
    "# Label Encoding for Total_Stops\n",
    "label_encoder = LabelEncoder()\n",
    "df['Total_Stops'] = label_encoder.fit_transform(df['Total_Stops'])\n",
    "\n",
    "# Word2Vec for Additional_Info\n",
    "additional_info_sentences = df['Additional_Info'].apply(lambda x: x.split()).tolist()\n",
    "word2vec_model = Word2Vec(sentences=additional_info_sentences, vector_size=50, window=3, min_count=1, workers=4)\n",
    "df['Additional_Info_Word2Vec'] = df['Additional_Info'].apply(\n",
    "    lambda x: np.mean([word2vec_model.wv[word] for word in x.split() if word in word2vec_model.wv], axis=0)\n",
    "    if len(x.split()) > 0 else np.zeros(50)\n",
    ")\n",
    "\n",
    "# Dropping columns that are no longer needed\n",
    "df.drop(['Date_of_Journey', 'Dep_Time', 'Arrival_Time', 'Duration', 'Additional_Info'], axis=1, inplace=True)\n",
    "\n",
    "# Preprocessing for categorical data and feature scaling\n",
    "categorical_cols = ['Airline', 'Route']\n",
    "numerical_cols = ['Journey_Day', 'Journey_Month', 'Dep_Hour', 'Dep_Minute', 'Arrival_Hour', 'Arrival_Minute', 'Duration_Minutes', 'Total_Stops']\n",
    "word2vec_col = ['Additional_Info_Word2Vec']\n",
    "\n",
    "# Pipeline for categorical features (with OneHotEncoder)\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Pipeline for Source and Destination (with TF-IDF Vectorizer)\n",
    "tfidf_pipeline = Pipeline(steps=[\n",
    "    ('tfidf', TfidfVectorizer())\n",
    "])\n",
    "\n",
    "# Pipeline for numerical features\n",
    "numerical_pipeline = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# ColumnTransformer to apply transformations to appropriate columns\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('num', numerical_pipeline, numerical_cols),\n",
    "    ('cat', categorical_pipeline, categorical_cols),\n",
    "    ('tfidf_src', TfidfVectorizer(), 'Source'),\n",
    "    ('tfidf_dest', TfidfVectorizer(), 'Destination')\n",
    "])\n",
    "\n",
    "# Defining the pipeline with preprocessing and model\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', LinearRegression())\n",
    "])\n",
    "\n",
    "# Splitting the data into training and test sets\n",
    "X = df.drop('Price', axis=1)\n",
    "y = df['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Fitting the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluating the model\n",
    "score = pipeline.score(X_test, y_test)\n",
    "print(f\"Model R^2 score: {score}\")\n",
    "\n",
    "# Checking the transformation on the first row\n",
    "X_transformed = pipeline.named_steps['preprocessor'].transform(X.head(1))\n",
    "\n",
    "# Debugging: Print the transformed output\n",
    "print(f\"Transformed output:\\n{X_transformed}\")\n",
    "\n",
    "# Check the shape of the transformed data\n",
    "print(f\"Shape of transformed data: {X_transformed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256bb698",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
